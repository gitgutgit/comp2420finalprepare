

1.scatter 그리기와 값 추출


plt.figure(figsize=(12,8))
plt.scatter(Au_data['N_surface'], Au_data['Formation_E'])
plt.title("Correlation between 'N_surface' and 'Formation_E'")
plt.xlabel('N_surface')
plt.ylabel('Formation_E (eV)')
plt.show()

print("Correlation Coeficient: ",Au_data['Formation_E'].corr(Au_data["N_surface"]))


2.Correlation 항목별 추출

기존방식
print(df["T"].corr(df["Formation_E"]))
print(df["tau"].corr(df["Formation_E"]))
print(df["Volume"].corr(df["Formation_E"]))
print(df["Avg_bonds"].corr(df["Formation_E"]))
print(df["angle_avg"].corr(df["Formation_E"]))

print("Pearson Correlation Coefficient with 'Formation_E':")
Au_data[['T', 'tau', 'Volume', 'Avg_bonds', 'angle_avg','Formation_E']].corr(method='pearson')['Formation_E'] 
한번에 추출법

3.Linear regression

# YOUR CODE HERE
# drop rows will null value
q231 = Au_data[['Volume', 'Formation_E']].dropna()
# fit the model
lr231 = LinearRegression().fit(q231[['Volume']], q231['Formation_E'])

plt.figure(figsize=(12,8))
plt.scatter(q231['Volume'], q231['Formation_E'], label='Actual Data')
x = np.array([0, q231['Volume'].max()])
plt.plot(x, lr231.predict(x.reshape(-1,1)), c='r', linewidth=2, label='Model')
plt.xlabel("'Volume' (m^3)")
plt.ylabel("'Formation_E' (eV)")
plt.title('Volumn vs Formation_E and with Trained Model')
plt.legend(loc='best')
plt.plot()


3.2  LINER REGRESSION 에서 정수값인 아닌 요소 가있을떄 또는 뭐 A  B C D 타입 이렇게 문자 종류일떈 0001 0010 등 인코딩해서하느넙
다만 여기서 first_drope한이유 중 나머지 요소가 다 0이면 코너라는 뜻이니까.. 굳이 안없앨려면 안없애도 될듯?? 근데 효율성
여기선 concat안하는이유 전부 요소가안인 lotconfig만 따지니까 저거 그대롴쓰면됨

https://piazza.com/class/le586uf3fin6sy/post/714 참고 label encode dummy등
이방식은 label 방식 중간고사
Au_data['GTAvg'] = Au_data['Formation_E']>=avg   이렇게 TRURE /FALSE가 나오게만들었으면
Au_data['GTAvg'] = Au_data['GTAvg'].astype(str).map({'False': 'No', 'True':'Yes'})  <= 근데 사실 이게 여기서필요한진모르겠음


다른 one hot 방식 이건 파이널2022방식
one_hot_vc = pd.get_dummies(q4_df[[ 'Fuel Type']])
q4_df = q4_df.drop([ 'Fuel Type'], 1)        fuel type을 여러종류 나눴으니까 그냥 fueltype자체는드랍하고
q4_df = pd.concat((q4_df,one_hot_vc),1)           one_hot_vc를 concat한다! 아래 lab은 그냥 x가 그것만써서 concat을안한거


이방식은 one hot 방식 (lab4)
# we'll do the encoding for you today
X = houses["LotConfig"]
enc_X = pd.get_dummies(X, drop_first=True)             # create K-1 new columns, ignore the first level
dropped_level = np.sort(houses["LotConfig"].unique())[0]

# fitting the regression
y = houses["SalePrice"]
lr = LinearRegression()
model = lr.fit(enc_X, y)                               # same as before, using an encoded variable

# outputting the results
categories = enc_X.columns
print("Intercept is", int(model.intercept_))
print("Coefficient for", dropped_level, "is 0 (because we dropped the first dummy level)")
for i, coef in enumerate(model.coef_):
    print("Coefficient for", categories[i], "is", int(coef))


이건 리코드하는법 아래처럼 다시정렬
# TODO: recode LotShape to be numeric
# we can use factorize to do this easily
houses["LotShapeRecode"] = pd.factorize(houses["LotShape"])[0]
houses["LotShapeRecode"].value_counts()    # compare to previous value counts

0    925
1    484
2     41
3     10 다시 이렇게 뜨는법





3.3 DISCUSS 어떤 것이 CHARATERISE MODEL인지 (EVALUATE MODEL)
r^2 와 RMSE  둘중하나만해도됨
# YOUR CODE HERE
true_y = q231["Formation_E"]
print("Volume model:")
print(f'Testing R^2 value: {lr231.score(np.array(q231["Volume"]).reshape(-1,1), true_y)}')
print(f'Testing RMSE value: {np.sqrt(mean_squared_error(lr231.predict(np.array(q231["Volume"]).reshape(-1,1)), true_y))}')

print("Total number of atoms:")
print(f'Testing R^2 value: {lr232.score(np.array(q232["N_total"]).reshape(-1,1), true_y)}')
print(f'Testing RMSE value: {np.sqrt(mean_squared_error(lr232.predict(np.array(q232["N_total"]).reshape(-1,1)), true_y))}')



     
=== 


4. TREE!! TREE는 요소들과 결정요소 가필요  GDP >500 이냐  등 YES NO로 (0 1로하든가)
# YOUR CODE HERE 중간고사 방법 은 label encode 방식
avg=Au_data['Formation_E'].mean()
Au_data['GTAvg'] = Au_data['Formation_E']>=avg
Au_data['GTAvg'] = Au_data['GTAvg'].astype(str).map({'False': 'No', 'True':'Yes'})
q24 = Au_data[['Volume', 'N_bulk', 'N_bonds', 'R_avg', 'HCP', 'GTAvg']]

#yes no방식으로 했다면 label encoder 하면됨 다만 q24[원래내용] = le 이렇게해서 적용시킬 데이터프레임에 잘 적용시켱냐함
le = LabelEncoder()
q24['GTAvg'] = le.fit_transform(q24['GTAvg'])









#당연히 train,test
train, test = train_test_split(q24, test_size=.2)
# 이젠 d.t 및 !! -1하는이유  gtavg 0 1 판독해둔걸 뺴고 넣어야하니까 x에는 y에 그 나머지하나를 넣으면됨 그래서 아래는 거의 고정느낌)











ac랑 f1뽑는법
from sklearn.metrics import accuracy_score,f1_score      print("Accuracy score: ",accuracy_score(test_y,pred)))  print("F1 score: ",f1_score(test_y,pred)) 


dt = DecisionTreeClassifier(max_depth=3).fit(train.iloc[:,:-1], train.iloc[:,-1])
ypredtrain = dt.predict(train.iloc[:,:-1])
ypredtest = dt.predict(test.iloc[:,:-1])
print('Training Mean Accuracy:', dt.score(train.iloc[:,:-1], train.iloc[:,-1]))
print('Testing Mean Accuracy:', dt.score(test.iloc[:,:-1], test.iloc[:,-1]))
print('Training Recall:', recall_score(train.iloc[:,-1], ypredtrain))
print('Testing Recall', recall_score(test.iloc[:,-1], ypredtest))
print('Training Precision:', precision_score(train.iloc[:,-1], ypredtrain))
print('Testing Precision', precision_score(test.iloc[:,-1], ypredtest))





4.2 gini impurity, gain

# YOUR CODE HERE
# Part 2 COMP2420
이게 어디서든 쓸수있는 유용한 gini impurity 와 gain
def gini_impurity(y):
    label_gini = 0
    for l in y.unique():
        p = sum(l==y)/len(y)
        label_gini += p * (1-p)
    return label_gini


def calculate_info_gain(X, y, dt):
    
    leaf_id = dt.apply(X)
    class_prob = dt.predict_proba(X)[:,0]

    dt_gini = 0
    for leaf_node in np.unique(leaf_id):
        num_of_sample = sum(leaf_id == leaf_node)
        p = np.sum(class_prob[leaf_id == leaf_node], axis=0)/num_of_sample
        for l in y.unique():
            dt_gini += p * (1-p) * num_of_sample/len(X)
    label_gini = gini_impurity(y)
    print(f'Gini Impurity of the Labels: {label_gini:.4f}')
    print(f'Total Gini gain: {label_gini:.4f} - {dt_gini:.4f} = {(label_gini-dt_gini):.4f}')
   
예시 max_depth 가 고정이면 for loop 굳이안해도됨

for max_depth in [3, 4]:
    print('max_depth: {max_depth}: ')
    dt_2 = DecisionTreeClassifier(max_depth=max_depth).fit(train_x, train_y)
    print('Train Set: ')
    calculate_info_gain(train_x, train_y, dt_2)
    print('Test Set: ')
    calculate_info_gain(test_x, test_y, dt_2)



kmean로만든 clustering의 한계
KMeans is relatively simple to implement and generalizes to different clusters. However, a limitation is tht we have to choose k manually and the outcome is dependent on the inital values of the cluster centers.

5. Clustering k값은 문제에 주어진것(또는 하이퍼 페러미터)

# YOUR CODE HERE
q25 = Au_data[['Total_E', 'Formation_E']]
km = KMeans(n_clusters=5).fit(q25)

colors=["red","blue","green","purple","orange"]
clus = km.predict(q25)

NON REDUCED 버전 
# plotting with different coloured clusters and showing cluster centres
plt.figure(figsize=(12,8))
for i in range(5):
    plt.scatter(q25['Total_E'][clus==i], q25['Formation_E'][clus==i], label=i, c=colors[i], alpha=0.5)
plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], label='Cluster Centers', c="black", s=200)
plt.title("K-Means Clustering",size=20)
plt.xlabel('Total_E', size=16)
plt.ylabel('Formation_E', size=16)
plt.legend()
plt.show()

5.12 REDUCED가 필요한버전  SUBPLOT경우에는 여러개 그려야할경우에만
#If we are using numerical data
fig = plt.figure(figsize=(15,5))
colors=["red","green","blue","brown","purple"] 색깔은 꼴리는대로

pca = PCA(n_components=2)
poke_reduced=pd.DataFrame(pca.fit_transform(poke_data.iloc[:,3:-1])) 

#Five cluster
km5_model = KMeans(n_clusters=5)
km5_model.fit(poke_reduced)

ax1 = fig.add_subplot(121)
for i in range(np.max(km5_model.labels_)+1):
    plt.scatter(poke_reduced[km5_model.labels_==i].iloc[:,0], poke_reduced[km5_model.labels_==i].iloc[:,1], label=i, c=colors[i], alpha=0.5)
plt.scatter(km5_model.cluster_centers_[:,0], km5_model.cluster_centers_[:,1], label='Cluster Centers', c="black", s=200)
plt.title("K-Means 5 Cluster of Pokemon",size=20)
plt.xlabel("Principle Component 1", size=16)
plt.ylabel("Principle Component 2", size=16)
plt.legend()




5.2 센터가 필요없을떄 그리고 pca를해서 2차원으로햇거나 또는 어차피 2개 일떄

# YOUR CODE HERE

pca = PCA(n_components=2)
cluter_df = q3q4_df[['Fuel Consumption Comb (L/100 km)', 'CO2 Emissions(g/km)', 'Engine Size(L)', 'Cylinders']]
km_model = KMeans(n_clusters=5)
reduced_df = pca.fit_transform(cluter_df)
km_model.fit(reduced_df)

sns.scatterplot(x=reduced_df[:, 0], y=reduced_df[:, 1], hue = map(lambda x : 'Cluster '+str(x), km_model.labels_ + 1))
plt.title('KMeans K = 5')
plt.show()



=============elbow sihooullete hyper..
#6.1 elbow

# YOUR CODE HERE
inertias = []
K = range(2, 10)
  
for k in K:
    # Building and fitting the model
    kmeanModel = KMeans(n_clusters=k).fit(q25)
    inertias.append(kmeanModel.inertia_)

plt.figure(figsize=(12,8))
plt.plot(K, inertias)
plt.xlabel('K', size=16)
plt.ylabel('Total Within Cluster Sum of Squares', size=16)
plt.show()


#6.2 sihoullete

# YOUR CODE HERE
def disSum(d, clus):
    return np.sum([pairwise_distances(d.reshape(1,-1), clus)])
def disMean(d, clus):
    return disSum(d,clus)/len(clus)
def sil(d, label, clusList):
    a = disSum(d, clusList[label]) / (len(clusList[label])-1)
    b = np.min([disMean(d, clusList[i]) for i in range(len(clusList)) if i!=label])
    return (b-a)/np.max([a,b])

sils = []
K = range(2, 10)

for k in K:
   # Building and fitting the model
    kmeanModel = KMeans(n_clusters=k).fit(q25)
    pred = kmeanModel.predict(q25)
    clusList = [q25[pred==i] for i in range(k)]
    sils.append(np.mean([sil(d, label, clusList) for d, label in zip(q25.values, pred)]))

plt.figure(figsize=(12,8))
plt.plot(K, sils)
plt.xlabel('Number of Cluster k', size=16)
plt.ylabel('Average Silhouette Width', size=16)
plt.show()


